{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charge Classifier: Data Analysis, Baseline Evaluation, and Fine-Tuning\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jli-together/colab-test/blob/main/Charge_Classifier_Analysis_and_Finetuning.ipynb)\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. **Data Analysis**: Explore the charge classifier dataset\n",
    "2. **Train/Val Split**: Create stratified splits for training and evaluation\n",
    "3. **Baseline Evaluation**: Test baseline models on the validation set\n",
    "4. **Fine-Tuning**: Train a specialized classifier model\n",
    "5. **Comparison**: Compare fine-tuned model with baseline performance\n",
    "\n",
    "**Dataset**: `CHARGE_CLASSIFIER_JUDGE_251111_NO_SYNTH_COUNTY_CRIM_MINIMAL_PROMPT_train.jsonl`\n",
    "\n",
    "**Key Features**:\n",
    "- Testing mode (10 examples) to verify workflow before full run\n",
    "- Full mode (8k+ examples) for complete analysis\n",
    "- Baseline vs fine-tuned model comparison"
   ],
   "id": "intro"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Installation"
   ],
   "id": "setup"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install -qU together datasets matplotlib seaborn pandas tqdm scikit-learn"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "install"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import together\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# Initialize Together client\n",
    "client = together.Client()\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Configuration\n",
    "TESTING_MODE = True  # Set to False for full dataset (8k+ samples)\n",
    "TESTING_SAMPLE_SIZE = 10  # Number of examples to use in testing mode\n",
    "\n",
    "# Option 1: Use uploaded file from Together AI (for Colab/cloud environments)\n",
    "USE_UPLOADED_FILE = True  # Set to True if file is already uploaded to Together AI\n",
    "UPLOADED_FILE_ID = \"file-67114292-64db-484b-ad28-53b764c1566d\"  # Your uploaded file ID\n",
    "\n",
    "# Option 2: Use local file path (for local environments)\n",
    "DATASET_PATH = \"CHARGE_CLASSIFIER_JUDGE_251111_NO_SYNTH_COUNTY_CRIM_MINIMAL_PROMPT_train.jsonl\"\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"   Testing Mode: {TESTING_MODE}\")\n",
    "if TESTING_MODE:\n",
    "    print(f\"   Sample Size: {TESTING_SAMPLE_SIZE} examples\")\n",
    "else:\n",
    "    print(f\"   Full Dataset Mode\")\n",
    "if USE_UPLOADED_FILE:\n",
    "    print(f\"   Using uploaded file from Together AI: {UPLOADED_FILE_ID}\")\n",
    "else:\n",
    "    print(f\"   Using local file: {DATASET_PATH}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "imports"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Load and Explore Dataset"
   ],
   "id": "load"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "if USE_UPLOADED_FILE:\n",
    "    # Download file from Together AI\n",
    "    print(f\"Downloading dataset from Together AI (file ID: {UPLOADED_FILE_ID})...\")\n",
    "    local_file_path = \"downloaded_dataset.jsonl\"\n",
    "    try:\n",
    "        client.files.retrieve_content(UPLOADED_FILE_ID, output=local_file_path)\n",
    "        print(f\"‚úì Downloaded file to {local_file_path}\")\n",
    "        DATASET_PATH = local_file_path\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error downloading file: {str(e)}\")\n",
    "        print(\"   Falling back to local file path\")\n",
    "        USE_UPLOADED_FILE = False\n",
    "\n",
    "if not USE_UPLOADED_FILE:\n",
    "    print(f\"Loading dataset from {DATASET_PATH}...\")\n",
    "\n",
    "data = []\n",
    "with open(DATASET_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"‚úì Loaded {len(data)} examples\")\n",
    "\n",
    "# Apply testing mode if enabled\n",
    "if TESTING_MODE:\n",
    "    print(f\"\\nüß™ TESTING MODE: Using first {TESTING_SAMPLE_SIZE} examples\")\n",
    "    data = data[:TESTING_SAMPLE_SIZE]\n",
    "    print(f\"‚úì Reduced to {len(data)} examples for testing\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"  Total examples: {len(df)}\")\n",
    "print(f\"\\nFirst example:\")\n",
    "print(json.dumps(data[0], indent=2))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "load_data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Data Analysis"
   ],
   "id": "analysis"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyze completion distribution\n",
    "completion_counts = df['completion'].value_counts()\n",
    "completion_pct = df['completion'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä COMPLETION DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCounts:\")\n",
    "for label, count in completion_counts.items():\n",
    "    print(f\"  {label}: {count} ({completion_pct[label]:.2f}%)\")\n",
    "\n",
    "# Visualize distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "completion_counts.plot(kind='bar', ax=ax1, color=['#2ecc71', '#e74c3c'])\n",
    "ax1.set_title('Completion Distribution (Counts)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Completion Label', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "completion_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%',\n",
    "                       colors=['#2ecc71', '#e74c3c'], startangle=90)\n",
    "ax2.set_title('Completion Distribution (Percentages)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Dataset is {'balanced' if abs(completion_pct.iloc[0] - 50) < 10 else 'imbalanced'}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "completion_dist"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyze prompt characteristics\n",
    "df['prompt_length'] = df['prompt'].str.len()\n",
    "df['prompt_word_count'] = df['prompt'].str.split().str.len()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä PROMPT STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nPrompt Length (characters):\")\n",
    "print(f\"  Mean: {df['prompt_length'].mean():.1f}\")\n",
    "print(f\"  Median: {df['prompt_length'].median():.1f}\")\n",
    "print(f\"  Min: {df['prompt_length'].min()}\")\n",
    "print(f\"  Max: {df['prompt_length'].max()}\")\n",
    "\n",
    "print(f\"\\nPrompt Word Count:\")\n",
    "print(f\"  Mean: {df['prompt_word_count'].mean():.1f}\")\n",
    "print(f\"  Median: {df['prompt_word_count'].median():.1f}\")\n",
    "print(f\"  Min: {df['prompt_word_count'].min()}\")\n",
    "print(f\"  Max: {df['prompt_word_count'].max()}\")\n",
    "\n",
    "# Visualize prompt length distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "df['prompt_length'].hist(bins=30, ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Prompt Length Distribution (Characters)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Character Count', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "df['prompt_word_count'].hist(bins=30, ax=ax2, color='lightcoral', edgecolor='black')\n",
    "ax2.set_title('Prompt Word Count Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Word Count', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "prompt_stats"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyze by completion label\n",
    "print(\"=\"*80)\n",
    "print(\"üìä STATISTICS BY COMPLETION LABEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for label in df['completion'].unique():\n",
    "    label_df = df[df['completion'] == label]\n",
    "    print(f\"\\n{label} ({len(label_df)} examples):\")\n",
    "    print(f\"  Avg prompt length: {label_df['prompt_length'].mean():.1f} chars\")\n",
    "    print(f\"  Avg word count: {label_df['prompt_word_count'].mean():.1f} words\")\n",
    "\n",
    "# Box plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df.boxplot(column='prompt_length', by='completion', ax=ax)\n",
    "ax.set_title('Prompt Length by Completion Label', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Completion Label', fontsize=12)\n",
    "ax.set_ylabel('Prompt Length (characters)', fontsize=12)\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "label_stats"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Create Train/Validation Split"
   ],
   "id": "split"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create stratified train/validation split\n",
    "# Use 80/20 split for train/val\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Creating stratified train/validation split...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Stratified split to maintain label distribution\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df['completion']  # Maintain label distribution\n",
    ")\n",
    "\n",
    "print(f\"‚úì TRAIN dataset: {len(train_df)} examples ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"‚úì VALIDATION dataset: {len(val_df)} examples ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show distribution in each split\n",
    "print(f\"\\nüìä Train set distribution:\")\n",
    "train_dist = train_df['completion'].value_counts()\n",
    "for label, count in train_dist.items():\n",
    "    pct = count / len(train_df) * 100\n",
    "    print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Validation set distribution:\")\n",
    "val_dist = val_df['completion'].value_counts()\n",
    "for label, count in val_dist.items():\n",
    "    pct = count / len(val_df) * 100\n",
    "    print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Verify no overlap\n",
    "train_ids = set(train_df.index)\n",
    "val_ids = set(val_df.index)\n",
    "overlap = train_ids & val_ids\n",
    "print(f\"\\n‚úì Verification: {len(overlap)} overlapping samples (should be 0)\")\n",
    "print(\"=\"*80)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "train_val_split"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Baseline Evaluation\n",
    "\n",
    "We'll evaluate baseline models on the validation set to establish a performance baseline before fine-tuning."
   ],
   "id": "baseline"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define baseline models to evaluate\n",
    "BASELINE_MODELS = [\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "]\n",
    "\n",
    "# Judge system prompt for classification\n",
    "JUDGE_SYSTEM_PROMPT = \"\"\"You are a legal charge classifier. Your task is to evaluate whether a charge classification is Valid or Error.\n",
    "\n",
    "Given:\n",
    "- A charge description\n",
    "- A state\n",
    "- A classification output\n",
    "\n",
    "Determine if the classification is Valid (correct) or Error (incorrect).\n",
    "\n",
    "Respond with only \"Valid\" or \"Error\".\"\"\"\n",
    "\n",
    "print(\"Selected Baseline Models:\")\n",
    "for i, model in enumerate(BASELINE_MODELS, 1):\n",
    "    print(f\"  {i}. {model}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "baseline_models"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare validation data for evaluation\n",
    "def prepare_for_evaluation(df_split):\n",
    "    \"\"\"Convert DataFrame to list of dicts for evaluation.\"\"\"\n",
    "    eval_data = []\n",
    "    for idx, row in df_split.iterrows():\n",
    "        eval_data.append({\n",
    "            'id': str(idx),\n",
    "            'prompt': row['prompt'],\n",
    "            'ground_truth': row['completion']\n",
    "        })\n",
    "    return eval_data\n",
    "\n",
    "val_eval_data = prepare_for_evaluation(val_df)\n",
    "\n",
    "print(f\"‚úì Prepared {len(val_eval_data)} validation examples for baseline evaluation\")\n",
    "print(f\"\\nSample evaluation data:\")\n",
    "print(json.dumps(val_eval_data[0], indent=2))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "prepare_eval"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run baseline evaluation\n",
    "print(\"Starting baseline evaluation on validation set...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for model in BASELINE_MODELS:\n",
    "    print(f\"\\nüîÑ Evaluating {model}...\")\n",
    "    model_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for example in tqdm(val_eval_data, desc=f\"  {model.split('/')[-1]}\"):\n",
    "        try:\n",
    "            # Call the model\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": example['prompt']}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=10\n",
    "            )\n",
    "\n",
    "            predicted = response.choices[0].message.content.strip()\n",
    "            ground_truth = example['ground_truth']\n",
    "\n",
    "            # Normalize predictions (handle variations)\n",
    "            predicted_normalized = predicted.upper()\n",
    "            if 'VALID' in predicted_normalized:\n",
    "                predicted_normalized = 'Valid'\n",
    "            elif 'ERROR' in predicted_normalized:\n",
    "                predicted_normalized = 'Error'\n",
    "            else:\n",
    "                predicted_normalized = predicted  # Keep original if unclear\n",
    "\n",
    "            is_correct = (predicted_normalized == ground_truth)\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            model_results.append({\n",
    "                'id': example['id'],\n",
    "                'prompt': example['prompt'],\n",
    "                'ground_truth': ground_truth,\n",
    "                'predicted': predicted,\n",
    "                'predicted_normalized': predicted_normalized,\n",
    "                'correct': is_correct\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Error on example {example['id']}: {str(e)}\")\n",
    "            model_results.append({\n",
    "                'id': example['id'],\n",
    "                'prompt': example['prompt'],\n",
    "                'ground_truth': example['ground_truth'],\n",
    "                'predicted': 'ERROR',\n",
    "                'predicted_normalized': 'Error',\n",
    "                'correct': False\n",
    "            })\n",
    "            total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    baseline_results[model] = {\n",
    "        'results': model_results,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total\n",
    "    }\n",
    "\n",
    "    print(f\"  ‚úì Accuracy: {accuracy*100:.2f}% ({correct}/{total})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"‚úì Baseline evaluation complete\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "run_baseline"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize baseline results\n",
    "print(\"=\"*80)\n",
    "print(\"üìä BASELINE EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_data = []\n",
    "for model, metrics in baseline_results.items():\n",
    "    model_name = model.split('/')[-1]\n",
    "    results_data.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': metrics['accuracy'] * 100,\n",
    "        'Correct': metrics['correct'],\n",
    "        'Total': metrics['total']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_data).sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nResults Summary:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.barh(results_df['Model'], results_df['Accuracy'], color='steelblue')\n",
    "ax.set_xlabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Baseline Model Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, acc) in enumerate(zip(bars, results_df['Accuracy'])):\n",
    "    ax.text(acc + 1, i, f'{acc:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best baseline\n",
    "best_baseline = results_df.iloc[0]\n",
    "print(f\"\\nüèÜ Best Baseline Model: {best_baseline['Model']}\")\n",
    "print(f\"   Accuracy: {best_baseline['Accuracy']:.2f}%\")\n",
    "print(f\"   Correct: {int(best_baseline['Correct'])}/{int(best_baseline['Total'])}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "baseline_viz"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Fine-Tuning\n",
    "\n",
    "Now we'll fine-tune a model on the training data to improve classification performance.\n",
    "\n",
    "**Note**: If you have already uploaded a file to Together AI (file ID: `file-67114292-64db-484b-ad28-53b764c1566d`), you can:\n",
    "1. Use it directly if it's already in fine-tuning format (with \"messages\" field)\n",
    "2. Or convert the data below and upload new files in the correct format\n",
    "\n",
    "The notebook will automatically download the uploaded file for analysis and can use it for fine-tuning."
   ],
   "id": "finetune"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Prepare training data for fine-tuning\n",
    "# Format: chat completion format for Together AI\n",
    "# Note: If your uploaded file is already in fine-tuning format (with \"messages\" field),\n",
    "# you can skip this conversion and use the file ID directly in the fine-tuning job.\n",
    "\n",
    "def prepare_finetuning_data(df_split):\n",
    "    \"\"\"Convert DataFrame to fine-tuning format.\"\"\"\n",
    "    finetune_data = []\n",
    "    for idx, row in df_split.iterrows():\n",
    "        finetune_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": row['prompt']},\n",
    "                {\"role\": \"assistant\", \"content\": row['completion']}\n",
    "            ]\n",
    "        })\n",
    "    return finetune_data\n",
    "\n",
    "train_finetune_data = prepare_finetuning_data(train_df)\n",
    "val_finetune_data = prepare_finetuning_data(val_df)\n",
    "\n",
    "print(f\"‚úì Prepared fine-tuning data:\")\n",
    "print(f\"  Training examples: {len(train_finetune_data)}\")\n",
    "print(f\"  Validation examples: {len(val_finetune_data)}\")\n",
    "\n",
    "# Show sample\n",
    "print(f\"\\nüìã Sample fine-tuning example:\")\n",
    "print(json.dumps(train_finetune_data[0], indent=2))"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "prepare_finetune"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save fine-tuning data to JSONL files\n",
    "os.makedirs('finetune_data', exist_ok=True)\n",
    "\n",
    "train_file = 'finetune_data/train.jsonl'\n",
    "val_file = 'finetune_data/val.jsonl'\n",
    "\n",
    "with open(train_file, 'w') as f:\n",
    "    for example in train_finetune_data:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "with open(val_file, 'w') as f:\n",
    "    for example in val_finetune_data:\n",
    "        f.write(json.dumps(example) + '\\n')\n",
    "\n",
    "print(f\"‚úì Saved fine-tuning data:\")\n",
    "print(f\"  Training: {train_file} ({len(train_finetune_data)} examples)\")\n",
    "print(f\"  Validation: {val_file} ({len(val_finetune_data)} examples)\")\n",
    "\n",
    "# Get file sizes\n",
    "train_size = os.path.getsize(train_file) / (1024 * 1024)  # MB\n",
    "val_size = os.path.getsize(val_file) / (1024 * 1024)  # MB\n",
    "print(f\"\\nüìÇ File sizes:\")\n",
    "print(f\"  Training: {train_size:.2f} MB\")\n",
    "print(f\"  Validation: {val_size:.2f} MB\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "save_finetune"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Upload files to Together AI (or use existing uploaded file)\n",
    "print(\"Preparing fine-tuning files...\")\n",
    "print(\"=\"*80)\n",
    "print(\"Note: The uploaded file should be in fine-tuning format (with 'messages' field).\")\n",
    "print(\"If your uploaded file is in the original format (prompt/completion),\")\n",
    "print(\"the converted files below will be uploaded instead.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Option 1: Use the already uploaded file if it's already in fine-tuning format\n",
    "USE_UPLOADED_FILE_FOR_FINETUNING = False  # Set to True if uploaded file is in fine-tuning format\n",
    "\n",
    "if USE_UPLOADED_FILE_FOR_FINETUNING and USE_UPLOADED_FILE and UPLOADED_FILE_ID:\n",
    "    print(f\"Using already uploaded file: {UPLOADED_FILE_ID}\")\n",
    "    print(\"Note: This assumes the uploaded file is the training file.\")\n",
    "    print(\"If you need separate train/val files, upload them below.\")\n",
    "\n",
    "    # Use the uploaded file ID directly\n",
    "    class FileObj:\n",
    "        def __init__(self, file_id):\n",
    "            self.id = file_id\n",
    "    train_file_obj = FileObj(UPLOADED_FILE_ID)\n",
    "    val_file_obj = None  # Validation can be done separately\n",
    "\n",
    "    print(f\"‚úì Using uploaded training file: {UPLOADED_FILE_ID}\")\n",
    "    print(\"‚ö†Ô∏è Note: Using the same file for training. For proper validation, upload a separate validation file.\")\n",
    "else:\n",
    "    # Option 2: Upload converted files (original format -> fine-tuning format)\n",
    "    print(\"Converting data to fine-tuning format and uploading...\")\n",
    "    print(\"Uploading fine-tuning data to Together AI...\")\n",
    "    try:\n",
    "        train_file_obj = client.files.upload(train_file, check=True)\n",
    "        print(f\"‚úì Uploaded training file: {train_file_obj.id}\")\n",
    "\n",
    "        # Note: LoRA fine-tuning typically only needs training file\n",
    "        # Validation can be done separately after training\n",
    "        val_file_obj = client.files.upload(val_file, check=True)\n",
    "        print(f\"‚úì Uploaded validation file: {val_file_obj.id}\")\n",
    "\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úì Files uploaded successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error uploading files: {str(e)}\")\n",
    "        print(\"Note: You may need to upload files manually or check API credentials\")\n",
    "        train_file_obj = None\n",
    "        val_file_obj = None"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "upload_files"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create LoRA fine-tuning job\n",
    "# Select base model for fine-tuning\n",
    "FINETUNE_BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # Can be changed\n",
    "WANDB_API_KEY = os.getenv(\"WANDB_API_KEY\", None)  # Optional: set your wandb key\n",
    "\n",
    "print(f\"Creating LoRA fine-tuning job...\")\n",
    "print(f\"Base model: {FINETUNE_BASE_MODEL}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if train_file_obj:\n",
    "    try:\n",
    "        # Fine-tuning parameters\n",
    "        # LoRA fine-tuning parameters (matching LoRA_Finetuning&Inference.ipynb)\n",
    "        finetune_job = client.fine_tuning.create(\n",
    "            training_file=train_file_obj.id,\n",
    "            model=FINETUNE_BASE_MODEL,\n",
    "            train_on_inputs=\"auto\",\n",
    "            n_epochs=3,\n",
    "            n_checkpoints=1,\n",
    "            wandb_api_key=WANDB_API_KEY,\n",
    "            lora=True,  # Enable LoRA fine-tuning\n",
    "            warmup_ratio=0,\n",
    "            learning_rate=1e-5,\n",
    "            suffix=\"charge-classifier\",  # Custom suffix for model name\n",
    "        )\n",
    "\n",
    "        print(f\"‚úì Fine-tuning job created!\")\n",
    "        print(f\"  Job ID: {finetune_job.id}\")\n",
    "        print(f\"  Output model name: {finetune_job.output_name}\")\n",
    "        print(f\"\\n‚è≥ Fine-tuning in progress...\")\n",
    "        print(f\"   You can check status at: https://api.together.ai/jobs\")\n",
    "        print(f\"   Or use: client.fine_tuning.retrieve('{finetune_job.id}')\")\n",
    "\n",
    "        finetune_job_id = finetune_job.id\n",
    "        finetuned_model_name = finetune_job.output_name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error creating fine-tuning job: {str(e)}\")\n",
    "        print(\"\\nNote: Fine-tuning may require specific API permissions or account setup\")\n",
    "        finetune_job_id = None\n",
    "        finetuned_model_name = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping fine-tuning job creation (training file not uploaded)\")\n",
    "    finetune_job_id = None"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "create_finetune_job"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Wait for fine-tuning to complete (if job was created)\n",
    "if finetune_job_id:\n",
    "    print(\"Waiting for fine-tuning to complete...\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"üí° Tip: You can also check status at https://api.together.ai/jobs\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    import time\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            status = client.fine_tuning.retrieve(finetune_job_id)\n",
    "            print(f\"Status: {status.status}\")\n",
    "\n",
    "            if status.status == \"completed\":\n",
    "                print(f\"\\n‚úì Fine-tuning completed!\")\n",
    "                print(f\"  Fine-tuned model: {status.output_name}\")\n",
    "                print(f\"  For LoRA inference, use: {status.output_name}-adapter\")\n",
    "                finetuned_model = status.output_name\n",
    "                break\n",
    "            elif status.status == \"failed\":\n",
    "                print(f\"\\n‚úó Fine-tuning failed\")\n",
    "                print(f\"  Error: {getattr(status, 'error', 'Unknown')}\")\n",
    "                finetuned_model = None\n",
    "                break\n",
    "            else:\n",
    "                print(f\"  Progress: {getattr(status, 'progress', 'N/A')}\")\n",
    "                time.sleep(30)  # Check every 30 seconds\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking status: {str(e)}\")\n",
    "            time.sleep(30)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No fine-tuning job to wait for\")\n",
    "    print(\"üí° For testing purposes, you can manually set finetuned_model below\")\n",
    "    finetuned_model = None\n",
    "    # Uncomment and set if you have a fine-tuned model:\n",
    "    # finetuned_model = \"your-finetuned-model-name\""
   ],
   "execution_count": null,
   "outputs": [],
   "id": "wait_finetune"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Compare Fine-Tuned Model with Baseline\n",
    "\n",
    "Evaluate the fine-tuned model on the validation set and compare with baseline performance."
   ],
   "id": "compare"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate fine-tuned model (using LoRA adapter)\n",
    "if finetuned_model:\n",
    "    # For LoRA fine-tuning, use model_name + '-adapter'\n",
    "    lora_model_name = finetuned_model + \"-adapter\"\n",
    "    print(f\"Evaluating fine-tuned LoRA model: {lora_model_name}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    finetuned_results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for example in tqdm(val_eval_data, desc=\"Fine-tuned model\"):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=lora_model_name,  # Use LoRA adapter for inference\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": example['prompt']}\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=10\n",
    "            )\n",
    "\n",
    "            predicted = response.choices[0].message.content.strip()\n",
    "            ground_truth = example['ground_truth']\n",
    "\n",
    "            # Normalize predictions\n",
    "            predicted_normalized = predicted.upper()\n",
    "            if 'VALID' in predicted_normalized:\n",
    "                predicted_normalized = 'Valid'\n",
    "            elif 'ERROR' in predicted_normalized:\n",
    "                predicted_normalized = 'Error'\n",
    "            else:\n",
    "                predicted_normalized = predicted\n",
    "\n",
    "            is_correct = (predicted_normalized == ground_truth)\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            finetuned_results.append({\n",
    "                'id': example['id'],\n",
    "                'prompt': example['prompt'],\n",
    "                'ground_truth': ground_truth,\n",
    "                'predicted': predicted,\n",
    "                'predicted_normalized': predicted_normalized,\n",
    "                'correct': is_correct\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è Error on example {example['id']}: {str(e)}\")\n",
    "            finetuned_results.append({\n",
    "                'id': example['id'],\n",
    "                'prompt': example['prompt'],\n",
    "                'ground_truth': example['ground_truth'],\n",
    "                'predicted': 'ERROR',\n",
    "                'predicted_normalized': 'Error',\n",
    "                'correct': False\n",
    "            })\n",
    "            total += 1\n",
    "\n",
    "    finetuned_accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"\\n‚úì Fine-tuned model evaluation complete\")\n",
    "    print(f\"  Accuracy: {finetuned_accuracy*100:.2f}% ({correct}/{total})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No fine-tuned model available for evaluation\")\n",
    "    print(\"   Set finetuned_model variable or wait for fine-tuning to complete\")\n",
    "    finetuned_accuracy = None\n",
    "    finetuned_results = []"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "eval_finetuned"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare baseline vs fine-tuned model\n",
    "print(\"=\"*80)\n",
    "print(\"üìä BASELINE vs FINE-TUNED COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare comparison data\n",
    "comparison_data = []\n",
    "for model, metrics in baseline_results.items():\n",
    "    model_name = model.split('/')[-1]\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Type': 'Baseline',\n",
    "        'Accuracy': metrics['accuracy'] * 100\n",
    "    })\n",
    "\n",
    "if finetuned_accuracy is not None:\n",
    "    # Extract model name for display\n",
    "    model_display_name = finetuned_model.split('/')[-1] if finetuned_model else 'Fine-tuned'\n",
    "    comparison_data.append({\n",
    "        'Model': f\"{model_display_name} (LoRA)\",\n",
    "        'Type': 'Fine-tuned',\n",
    "        'Accuracy': finetuned_accuracy * 100\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Separate baseline and fine-tuned\n",
    "baseline_df = comparison_df[comparison_df['Type'] == 'Baseline']\n",
    "finetuned_df = comparison_df[comparison_df['Type'] == 'Fine-tuned']\n",
    "\n",
    "print(\"\\nBaseline Models:\")\n",
    "print(baseline_df[['Model', 'Accuracy']].to_string(index=False))\n",
    "\n",
    "if not finetuned_df.empty:\n",
    "    print(\"\\nFine-tuned Model:\")\n",
    "    print(finetuned_df[['Model', 'Accuracy']].to_string(index=False))\n",
    "\n",
    "    # Calculate improvement\n",
    "    best_baseline_acc = baseline_df['Accuracy'].max()\n",
    "    finetuned_acc = finetuned_df['Accuracy'].iloc[0]\n",
    "    improvement = finetuned_acc - best_baseline_acc\n",
    "\n",
    "    print(f\"\\nüìà Performance Comparison:\")\n",
    "    print(f\"  Best Baseline: {best_baseline_acc:.2f}%\")\n",
    "    print(f\"  Fine-tuned: {finetuned_acc:.2f}%\")\n",
    "    print(f\"  Improvement: {improvement:+.2f} percentage points\")\n",
    "\n",
    "    if improvement > 0:\n",
    "        print(f\"  ‚úÖ Fine-tuning improved performance by {improvement:.2f}%\")\n",
    "    elif improvement < 0:\n",
    "        print(f\"  ‚ö†Ô∏è Fine-tuning decreased performance by {abs(improvement):.2f}%\")\n",
    "    else:\n",
    "        print(f\"  ‚û°Ô∏è Fine-tuning maintained baseline performance\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot baseline models\n",
    "baseline_colors = ['steelblue'] * len(baseline_df)\n",
    "if not finetuned_df.empty:\n",
    "    finetuned_color = ['green'] if finetuned_acc >= best_baseline_acc else ['red']\n",
    "    colors = baseline_colors + finetuned_color\n",
    "else:\n",
    "    colors = baseline_colors\n",
    "\n",
    "bars = ax.barh(comparison_df['Model'], comparison_df['Accuracy'], color=colors)\n",
    "ax.set_xlabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Baseline vs Fine-Tuned Model Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xlim(0, 100)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, acc) in enumerate(zip(bars, comparison_df['Accuracy'])):\n",
    "    ax.text(acc + 1, i, f'{acc:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='steelblue', label='Baseline'),\n",
    "]\n",
    "if not finetuned_df.empty:\n",
    "    legend_elements.append(Patch(facecolor='green' if finetuned_acc >= best_baseline_acc else 'red',\n",
    "                                 label='Fine-tuned'))\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "compare_results"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detailed error analysis\n",
    "if finetuned_accuracy is not None and finetuned_results:\n",
    "    print(\"=\"*80)\n",
    "    print(\"üîç ERROR ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Compare errors between best baseline and fine-tuned\n",
    "    best_baseline_model = baseline_df.loc[baseline_df['Accuracy'].idxmax(), 'Model']\n",
    "    best_baseline_full_name = [m for m in baseline_results.keys() if best_baseline_model in m][0]\n",
    "    best_baseline_results = baseline_results[best_baseline_full_name]['results']\n",
    "\n",
    "    # Find examples where they differ\n",
    "    baseline_correct = {r['id']: r['correct'] for r in best_baseline_results}\n",
    "    finetuned_correct = {r['id']: r['correct'] for r in finetuned_results}\n",
    "\n",
    "    # Cases where fine-tuned is better\n",
    "    improved = [id for id in baseline_correct.keys()\n",
    "                if not baseline_correct[id] and finetuned_correct.get(id, False)]\n",
    "\n",
    "    # Cases where fine-tuned is worse\n",
    "    regressed = [id for id in baseline_correct.keys()\n",
    "                 if baseline_correct[id] and not finetuned_correct.get(id, False)]\n",
    "\n",
    "    print(f\"\\nüìä Comparison with best baseline ({best_baseline_model}):\")\n",
    "    print(f\"  Examples improved: {len(improved)}\")\n",
    "    print(f\"  Examples regressed: {len(regressed)}\")\n",
    "\n",
    "    if improved:\n",
    "        print(f\"\\n‚úÖ Examples where fine-tuned model improved:\")\n",
    "        for i, example_id in enumerate(improved[:3], 1):  # Show first 3\n",
    "            example = [r for r in val_eval_data if r['id'] == example_id][0]\n",
    "            print(f\"\\n  {i}. ID: {example_id}\")\n",
    "            print(f\"     Prompt: {example['prompt'][:150]}...\")\n",
    "            print(f\"     Ground truth: {example['ground_truth']}\")\n",
    "\n",
    "    if regressed:\n",
    "        print(f\"\\n‚ö†Ô∏è Examples where fine-tuned model regressed:\")\n",
    "        for i, example_id in enumerate(regressed[:3], 1):  # Show first 3\n",
    "            example = [r for r in val_eval_data if r['id'] == example_id][0]\n",
    "            print(f\"\\n  {i}. ID: {example_id}\")\n",
    "            print(f\"     Prompt: {example['prompt'][:150]}...\")\n",
    "            print(f\"     Ground truth: {example['ground_truth']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Fine-tuned model results not available for error analysis\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "error_analysis"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. ‚úÖ **Data Analysis**: Explored the charge classifier dataset\n",
    "   - Analyzed completion distribution (Valid vs Error)\n",
    "   - Examined prompt characteristics\n",
    "   - Identified dataset statistics\n",
    "\n",
    "2. ‚úÖ **Train/Val Split**: Created stratified 80/20 split\n",
    "   - Maintained label distribution across splits\n",
    "   - Ensured no overlap between train and validation sets\n",
    "\n",
    "3. ‚úÖ **Baseline Evaluation**: Tested multiple baseline models\n",
    "   - Evaluated on validation set\n",
    "   - Established performance benchmarks\n",
    "\n",
    "4. ‚úÖ **Fine-Tuning**: Trained a specialized classifier\n",
    "   - Prepared data in proper format\n",
    "   - Created fine-tuning job\n",
    "   - Monitored training progress\n",
    "\n",
    "5. ‚úÖ **Comparison**: Compared fine-tuned vs baseline\n",
    "   - Measured performance improvement\n",
    "   - Analyzed error patterns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Full Dataset Run**: Set `TESTING_MODE = False` to run on full 8k+ dataset\n",
    "2. **Hyperparameter Tuning**: Experiment with different learning rates, epochs, batch sizes\n",
    "3. **Model Selection**: Try different base models for fine-tuning\n",
    "4. **Production Deployment**: Deploy the best-performing model for production use"
   ],
   "id": "summary"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
